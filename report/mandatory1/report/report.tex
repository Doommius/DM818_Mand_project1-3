%Template by Mark Jervelund - 2015 - mjerv15@student.sdu.dk

\documentclass[a4paper,10pt,titlepage]{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[document]{ragged2e}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{color}
\usepackage{datenumber}
\usepackage{venndiagram}
\usepackage{chngcntr}
\setdatetoday
\addtocounter{datenumber}{0} %date for dilierry standard is today
\setdatebynumber{\thedatenumber}
\date{}
\setcounter{secnumdepth}{0}
\pagestyle{fancy}
\fancyhf{}

\newcommand{\Z}{\mathbb{Z}}
\lhead{Parallel Computinge (DM818))}
\rhead{Mark Jervelund (Mjerv15)}
\rfoot{Page  \thepage \, of \pageref{LastPage}}
\counterwithin*{equation}{section}

\begin{document}
\begin{titlepage}
\centering
    \vspace*{9\baselineskip}
    \huge
    \bfseries
    1. Mandatory Assignment \\
    \normalfont 
    Mark Jervelund  \\
    (Mjerv15) \\
	\huge    
    Parallel Computing (DM818)  \\[4\baselineskip]
    \normalfont
	\includegraphics[scale=1]{SDU_Logo}
    \vfill\ 
    \vspace{5mm}
    IMADA \\

    \textbf{\datedate}  \bf{at 10} \\[2\baselineskip]
\end{titlepage}

\renewcommand{\thepage}{\roman{page}}% Roman numerals for page counter
\tableofcontents
\newpage
\setcounter{page}{1}
\renewcommand{\thepage}{\arabic{page}}
\section{Project description}
To describe the services an operating system provides to
users, processes, and other systems\\
To discuss the various ways of structuring an operating
system\\
To explain how operating systems are installed and
customized and how they boot\\
\newpage

\section{Introduction}
This report contains documentation for the first mandatory assignment for DM818 Parallel Computing, in the first assignment we were tasked with designing a high performance singled threaded matrix matrix multiplication program.

\subsection{Work load}
For the following projected i ended up working alone as i didn't really know anyone in the course as I'm a bachelor student, the people i knew from other courses were already in groups. 

\subsection{Issues}
There is currently a bug with the freeing the memory allocation in the program, this means that the program will work one iteration. I've tried bugging with gdb and Valgrind, but i haven't been able to figure out what the bug is yet. but i assume it's a overflow, or our of place memory happening in the B Array, see the appendix for Valgrind and a link to a post on stack overflow which could be the same as the issue I'm experiencing. 


\section{Design}
\subsection{Advice from lecturer}
For the designing the algorithm we were given 2 examples of how its normally naively implemented, a naive 3 for loop implementation, and a naive blocked version. \\ \vspace{5mm}
Furthermore we were pointed heavy towards the Goto paper on high performance matrix multiplication, which describes how you should design the blocking for the best possible performance. \\ \vspace{5mm}
and as a third pointer we were given a lecture by Jacob who explained how he implemented it, and what we should do to get the highest performance,
\subsection{Own design choices}

I made a design choice follow Jacobs implementation. this meant writing functions that pack the larger matrix into smaller sub-matrices that are then used for the matrix matrix multiplication. \\
\subsubsection{Cache}
For figuring out the block size i calculated the largest n by n matrix possible,
\begin{equation}
sqr((256*1024 byte)/64 bit) = 181.019335984
\end{equation}
as 181 isn't a good size when handing matrix blocking, i rounded down to nearest $2^n$ which is 128.
\begin{equation}
128 * 128 * 64 bit = 128 Kbytes
\end{equation}
We were also shown a equation to solve this. its 
\begin{equation}
8* (m_c*k_c+m_c*n_c+N_c*k_c) = 8* (128*128+128*4+4*128) = 139264 bytes ~ 136 Kbytes
\end{equation}
The reasoning for that we shouldn't use blocks larger then the cache is that we'll be fetching our data from main memory more often will will induce 100s or 1000s fold slowdown in execution time.\\
\subsubsection{Intrinsic}
I only decided on only implementing intrinsic for a core of 4x4, and just using a naive for loop implementation for the smaller blocks, as programming all the possible different blocks alone would take too much time.\\ 

\newpage
\section{Implementation}

\subsection{Square-dgemm}

The initial function square\_dgemm gets the 3 matrices and M in as the 4 arguments. M is directly saved as a global variable. \\
3 sub matrices are then reserved, Ablock, Bblock, and C, block. Ablock and Cblock and allocated in a way that is aligned in memory by 32-bytes. this is done so the intrinsic instructions will run faster. \\ \vspace{5mm}

The first for loop blocks B into blocks of of size KC * M(128 * size of matrix). The second for loop blocks A into blocks of 128 by 128 so we keep as much of A in cache as possible. \\

Prepare block is then called with the blocks we want to multiply.

\begin{lstlisting} 
void square_dgemm(int M, double *A, double *B, double *C) {
    lda = M;
    Ablock = (double *) _mm_malloc(MC * KC * sizeof(double), 32); //128*128
    Bblock = (double *) malloc(lda * KC * sizeof(double)); // M * 128
    Cblock = (double *) _mm_malloc(MC * NR * sizeof(double), 32); //128*4

    for (unsigned int k = 0; k < lda; k += KC) {
        packBBlock(B + k, std::min(KC, lda - k));
        for (unsigned int i = 0; i < lda; i += MC) {
            packAblock(A + i + k * lda, std::min(MC, lda - i), std::min(KC, lda - k));
            Prepare_block(C + i, std::min(MC, lda - i), lda, std::min(KC, lda - k));
        }
    }
    _mm_free(Ablock);
    free(Bblock);
    _mm_free(Cblock);
}
\end{lstlisting}

\subsection{Blocking functions}
The program has 2 block packing functions. and 1 unpacking, the 2 packing functions are quite close in usecase and what they do. PackA packs up to 128x128 of matrix A into Ablock, and Packb packs up to 128*lda into BlockB.\\ \vspace{5mm}
The last packing function is for unblocking is for unpacking Cblock into C

\subsection{do-block}
The do block functions 
\begin{lstlisting}
void Do_block(double *C, unsigned int M, unsigned int N, unsigned int K) {
    double *B = Bblock;
    for (unsigned int n = 0; n < N; n += NR) {
        for (unsigned int m = 0; m < M; m += MR) {
            unsigned int Max_M = std::min(NR, M - m);
            unsigned int Max_N = std::min(MR, N - n);
            if (Max_M == MR && Max_N == NR) {
                core_4_4(Ablock + m * K, B, Cblock + m, K);
            } else {
                core_dyn(Ablock + m, B, Cblock +m, M, N, K);
            }
        }
        B += NR * K;
        unpackCBlock(C + n * lda, M, std::min(NR, N - n));
    }
}
\end{lstlisting}


\section{testing}


\section{Comparison}

\section{Conclusion}


\newpage
\section{appendix}
\subsection{Goto paper }
https://dl.acm.org/citation.cfm?id=1356053 \\
\subsection{Code Protect}
https://www.codeproject.com/Articles/874396/Crunching-Numbers-with-AVX-and-AVX
\subsection{Valgrind result and stackoverflow post}

\begin{lstlisting}
https://stackoverflow.com/questions/2334352/why-do-i-get-a-sigabrt-here

==13219== Your program just tried to execute an instruction that Valgrind
==13219== did not recognise.  There are two possible reasons for this.
==13219== 1. Your program has a bug and erroneously jumped to a non-code
==13219==    location.  If you are running Memcheck and you just saw a
==13219==    warning about a bad jump, it's probably your program's fault.
==13219== 2. The instruction is legitimate but Valgrind doesn't handle it,
==13219==    i.e. it's Valgrind's fault.  If you think this is the case or
==13219==    you are not sure, please let us know and we'll try to fix it.
==13219== Either way, Valgrind will now raise a SIGILL signal which will
==13219== probably kill your program.
\end{lstlisting}


\section{other}
\begin{lstlisting}
 https://software.intel.com/sites/landingpage/IntrinsicsGuide/#techs=AVX_512&cats=Elementary%20Math%20Functions,Load 
	
https://www.codeproject.com/Articles/874396/Crunching-Numbers-with-AVX-and-AVX

_mm<bit_width>_<name>_<data_type> 

The parts of this format are given as follows:

<bit_width> identifies the size of the vector returned by the function. For 128-bit vectors, this is empty. For 256-bit vectors, this is set to 256.

<name> describes the operation performed by the intrinsic
<data_type> identifies the data type of the function's primary arguments


Data Type	Description
__m128i	128-bit vector containing 8 integers
__m128	128-bit vector containing 4 floats
__m128d	128-bit vector containing 2 doubles
__m256i	256-bit vector containing 16 integers
__m256	256-bit vector containing 8 floats
__m256d	256-bit vector containing 4 doubles
__m512i	512-bit vector containing 32 integers
__m512	512-bit vector containing 16 floats
__m512d	512-bit vector containing 8 doubles



ps - vectors contain floats (ps stands for packed single-precision)
pd - vectors contain doubles (pd stands for packed double-precision)
epi8/epi16/epi32/epi64 - vectors contain 8-bit/16-bit/32-bit/64-bit signed integers
epu8/epu16/epu32/epu64 - vectors contain 8-bit/16-bit/32-bit/64-bit unsigned integers
si128/si256 - unspecified 128-bit vector or 256-bit vector
m128/m128i/m128d/m256/m256i/m256d - identifies input vector types when they're different than the type of the returned vector

\end{lstlisting}

\section{notes from lecture with Jakob}

Our cache use, calculate it to be = to 256 or maybe a bit less.
$kc and mc should be large. n_c is less important.$
kc = 128 MC = 128, 
$8* (m_c*k_c+m_c*n_c+N_c+k_c)$


$avx_256 registers are called ymm0 to 15$
$avx_512 registers are called zmm0 to 31$


Make function for each size of slices..
128 = 4.4 
256 = 8.4 
512 = 8.8




































\end{document}
